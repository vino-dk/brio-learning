{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Brio Labyrinth with a Neural Network\n",
    "*Written by Tobias ?, Vinojan ?, Avi SZYCHTER*\n",
    "\n",
    "This notebook describes the neural network that we created in order to solve the brio labyrinth situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dectecting IPython...\n",
      "Matplotlib is configured to use  module://ipykernel.pylab.backend_inline\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input, LeakyReLU\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# Other dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from net_utils import plot_result, load_data_list, visLayer\n",
    "from preprocess import augmentData, crop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Load the data\n",
    "path = 'Annotated_Datasets/' # Configure to fit your file structure \n",
    "\n",
    "fileList = ['Annotated_Datasets/whole_roomANDwindowNoonlight.txt',\n",
    "           'Annotated_Datasets/whole_roomOnNoonWhiteGB.txt',\n",
    "           'Annotated_Datasets/whole_white_fromCenter.txt',\n",
    "           'Annotated_Datasets/whole_white_fromNorthEast.txt',\n",
    "           'Annotated_Datasets/whole_white_fromNorthWest.txt',\n",
    "           'Annotated_Datasets/whole_white_fromSouthWest.txt',\n",
    "           'Annotated_Datasets/whole_ledblue.txt',\n",
    "           'Annotated_Datasets/whole_ledgreen.txt']\n",
    "\n",
    "imgs_raw, anno_raw = load_data_list(path, fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 400, 470, 1)\n"
     ]
    }
   ],
   "source": [
    "# -- Augment the data by flipping and adjusting brightness. Also shuffle the data\n",
    "\n",
    "# -- Values for cropping out the board\n",
    "xmin = 180\n",
    "xmax = 650\n",
    "ymin = 50\n",
    "ymax = 450\n",
    "\n",
    "# -- Maximum value for random brightness adjust\n",
    "brightnessFactor = 20\n",
    "\n",
    "# -- Augment the data\n",
    "imgs_all, anno_all = augmentData(imgs_raw, anno_raw, xmin, xmax, ymin, ymax, brightnessFactor)\n",
    "print(imgs_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Split the data in training and validation\n",
    "splitTrainVal = round(imgs_all.shape[0]*0.80)\n",
    "\n",
    "x_train_no_norm = imgs_all[:splitTrainVal]\n",
    "x_val_no_norm = imgs_all[splitTrainVal:]\n",
    "\n",
    "y_train_no_norm = anno_all[:splitTrainVal]\n",
    "y_val_no_norm = anno_all[splitTrainVal:]\n",
    "\n",
    "print(x_train_no_norm.shape)\n",
    "print(x_val_no_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mean:  [112.60665356606584]\n",
      "x_std:  [53.08789144593608]\n",
      "x_train dimensions:  (120, 100, 100, 1)\n",
      "New mean of x:  [-0.52339815900651665]\n",
      "New std of x:  [0.70839702923776471]\n"
     ]
    }
   ],
   "source": [
    "# -- Normalize the training data\n",
    "x_mean = [112.60665356606584]\n",
    "x_std = [53.08789144593608]\n",
    "print('x_mean: ',x_mean)\n",
    "print('x_std: ',x_std)\n",
    "\n",
    "x_train = imgs_all - x_mean\n",
    "x_train /= x_std\n",
    "\n",
    "print('x_train dimensions: ',x_train.shape)\n",
    "print('New mean of x: ',[np.mean(x_train[:,:,:,0])])\n",
    "print('New std of x: ',[np.std(x_train[:,:,:,0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Normalize validation data\n",
    "x_val = x_val_no_norm.astype('float64') - x_mean\n",
    "x_val /= x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Scale annotations\n",
    "y_train = y_train_no_norm / [xmax-xmin,ymax-ymin]\n",
    "y_val = y_val_no_norm / [xmax-xmin,ymax-ymin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Initiliaze the tensorflow session and model\n",
    "model = None\n",
    "\n",
    "# -- Proposed to be necessary for running keras in jupyter\n",
    "session = K.get_session()\n",
    "if model is not None:\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Dropout and leaky relu parameter\n",
    "initRelu = Constant(value=0.1)\n",
    "drop = 0.05\n",
    "\n",
    "# -- Defining the input layer\n",
    "x_input = Input(shape=(200, 235, 1))\n",
    "\n",
    "# -- Defining the convolutional layers\n",
    "x = Conv2D(15, (3, 3), padding='same', activation=None, bias_initializer=initRelu)(x_input)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Conv2D(15, (3, 3), padding='same', activation=None, bias_initializer=initRelu)(x)\n",
    "\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(drop)(x)\n",
    "\n",
    "x = Conv2D(10, (3, 3), padding='same', activation=None, bias_initializer=initRelu)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Conv2D(10, (3, 3), padding='same', activation=None, bias_initializer=initRelu)(x)\n",
    "\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(drop)(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), padding='same', activation=None, bias_initializer=initRelu)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dropout(drop)(x)\n",
    "\n",
    "# -- Flatten output from convolutional layers and add regression head of 4 fully connected layers\n",
    "x = Flatten()(x)\n",
    "x = Dense(120)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dense(120)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dense(120)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dense(120)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dropout(drop)(x)\n",
    "\n",
    "# -- Define output layer \n",
    "y_out = Dense(2, activation='linear')(x)\n",
    "\n",
    "# -- Put the layers together as a model\n",
    "model = Model(inputs = x_input,outputs = y_out)\n",
    "\n",
    "print(model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- Define the optimiser for the training\n",
    "optim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# -- Compile the model\n",
    "model.compile(optimizer=optim, loss='mse')\n",
    "\n",
    "# -- Train the model\n",
    "train_history = model.fit(x_train,y_train, epochs=500, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Show the training and validation loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Model_big.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Load data for evaluation (annotated more precise)\n",
    "imgs_for_test, anno_for_test = load_data_list(path, ['Annotated_Datasets/whole_ledred.txt'])\n",
    "\n",
    "# -- Augment the data by flipping and adjusting brightness. Also shuffle the data\n",
    "xmin = 180\n",
    "xmax = 650\n",
    "ymin = 50\n",
    "ymax = 450\n",
    "brightnessFactor = 20\n",
    "\n",
    "imgs_aug_t, anno_aug_t = augmentData(imgs_for_test, anno_for_test, xmin, xmax, ymin, ymax, brightnessFactor, resize = (200,235))\n",
    "\n",
    "# -- Normalize the test data\n",
    "imgs_test_norm = imgs_aug_t.astype('float64') - x_mean\n",
    "imgs_test_norm /= x_std\n",
    "\n",
    "# -- Make predictions\n",
    "predicts = model.predict_on_batch(imgs_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014525151602356345\n"
     ]
    }
   ],
   "source": [
    "# -- Test the time it takes to make a prediction\n",
    "sample = imgs_test_norm[0]\n",
    "tick1 = time.clock()\n",
    "model.predict(sample)\n",
    "tick2 = time.clock()\n",
    "\n",
    "print(tick2-tick1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Print the predicted and actual positions and calculate the standard deviation in pixels\n",
    "losses = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    print(i, ' Actual pos:  ', y_test[i]*[xmax-xmin,ymax-ymin])\n",
    "    print(i, ' Predicted pos', predicts[i]*[xmax-xmin,ymax-ymin])\n",
    "    print(' ')\n",
    "    lossX2 = (y_test[i,0]*(xmax-xmin)-predicts[i,0]*(xmax-xmin))**2\n",
    "    lossY2 = (y_test[i,1]*(ymax-ymin)-predicts[i,1]*(ymax-ymin))**2\n",
    "    losses.append(lossX2 + lossY2)\n",
    "    \n",
    "std = (np.sum(losses)/(len(losses)-1))**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Print the standard deviation\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Print the highest error in pixels\n",
    "print((np.amax(losses))**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Visualise convolutional layers\n",
    "layer = 16\n",
    "\n",
    "model2 = Model(inputs = model.input, outputs = model.layers[layer].output)\n",
    "img = model2.predict(x_test)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(111, title = model2.layers[layer].name + ' filter:' + str(i))\n",
    "    ax.imshow(img[1,:,:,i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Visual validation of model showing the worst prediction\n",
    "image = np.argmax(losses)\n",
    "\n",
    "fig = plt.figure()\n",
    "    \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(imgs_aug_t[image,:,:,0].astype('uint8'), cmap= 'gray')\n",
    "ax.scatter(predicts[image,0]*235, \n",
    "           predicts[image,1]*200, \n",
    "           c=\"r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
